<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistical Learning:</title>
    <meta charset="utf-8" />
    <meta name="author" content="Prof.¬†Carlos Truc√≠os   FACC/UFRJ     ¬† ctruciosm.github.io  ¬† carlos.trucios@facc.ufrj.br " />
    <meta name="date" content="2021-06-18" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"xc9d0d8e0e2541a79a7e30d258926f5b","expires":14}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <script src="libs/js-cookie/js.cookie.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <script src="libs/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30}) })</script>
    <script src="libs/freezeframe/freezeframe.min.js"></script>
    <script src="libs/xaringanExtra-freezeframe/freezeframe-init.js"></script>
    <script id="xaringanExtra-freezeframe-options" type="application/json">{"selector":"img[src$=\"gif\"]","trigger":"click","overlay":false,"responsive":true,"warnings":true}</script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Statistical Learning:
## KNN: K-vizinhos mais pr√≥ximos
### Prof.¬†Carlos Truc√≠os <br> FACC/UFRJ <br><br> <a href="http://ctruciosm.github.io"> <i class="fa fa-desktop fa-fw"></i>¬† ctruciosm.github.io</a><br> <a href="mailto:carlos.trucios@facc.ufrj.br"><i class="fa fa-paper-plane fa-fw"></i>¬† carlos.trucios@facc.ufrj.br</a><br>
### Grupo de Estudos CIA, </br> ‚Äì Causal Inference and Analytics ‚Äì
### 2021-06-18

---






<div>
<style type="text/css">.xaringan-extra-logo {
width: 110px;
height: 128px;
z-index: 0;
background-image: url(imagens/CIA_logo.png);
background-size: contain;
background-repeat: no-repeat;
position: absolute;
top:1em;right:1em;
}
</style>
<script>(function () {
  let tries = 0
  function addLogo () {
    if (typeof slideshow === 'undefined') {
      tries += 1
      if (tries < 10) {
        setTimeout(addLogo, 100)
      }
    } else {
      document.querySelectorAll('.remark-slide-content:not(.title-slide):not(.inverse):not(.hide_logo)')
        .forEach(function (slide) {
          const logo = document.createElement('div')
          logo.classList = 'xaringan-extra-logo'
          logo.href = null
          slide.appendChild(logo)
        })
    }
  }
  document.addEventListener('DOMContentLoaded', addLogo)
})()</script>
</div>




## Um modelo intuitivo


&lt;img src="ISL_02_files/figure-html/unnamed-chunk-1-1.png" width="100%" /&gt;

--

O algoritmo **KNN** (k-vizinhos mais pr√≥ximos) trabalha com essa mesma l√≥gica! üÜí.

---
class: inverse, center, middle
# KNN
---

## KNN

.bg-washed-green.b--dark-green.ba.bw2.br3.shadow-5.ph4.mt5[
**k-Nearest Neighbors (kNN)** √© um algoritmo supervisionado de _Machine/Statistical Learning_ em que a predi√ß√£o para uma nova observa√ß√£o, `\(x_0\)`, depende dos seus `\(k\)`-vizinhos mais pr√≥ximos. 


O algoritmo atribuir√° a observa√ß√£o `\(x_0\)` ao grupo com maior frequ√™ncia entre seus `\(k\)`-vizinhos mais pr√≥ximos.
]



---

## KNN


&lt;img src="ISL_02_files/figure-html/unnamed-chunk-2-1.png" width="100%" /&gt;


---

## KNN: Primeiros passos


O _dataset_ `knn_exemplo` cont√©m informa√ß√µes de 15 observa√ß√µes dividias em dois grupos (A e B) e est√° disponivel [aqui](https://raw.githubusercontent.com/ctruciosm/ctruciosm.github.io/master/datasets/knn_exemplo.txt):


#### Hands-on:

1. Importar os dados para o R 
2. Fazer um gr√°fico de dispers√£o (cada grupo que tenha diferente cor)
3. Utilizaremos o algoritmo KNN para classificar a nova observa√ß√£o `new_obs = data.frame(V1 = 0.7, V2 = 0.4)` em algum dos grupos (A ou B)

--


```r
# Importando os dados
library(dplyr)
uri &lt;- "https://raw.githubusercontent.com/ctruciosm/ctruciosm.github.io/master/datasets/knn_exemplo.txt"
knn_exemplo &lt;- read.table(uri, header = TRUE)
glimpse(knn_exemplo)
```

---

## KNN: Primeiros passos






.panelset[
.panel[.panel-name[Gr√°fico]

&lt;img src="ISL_02_files/figure-html/unnamed-chunk-5-1.png" width="100%" /&gt;

]

.panel[.panel-name[C√≥digo]


```r
library(ggplot2)
ggplot(knn_exemplo) + 
  geom_point(aes(x = V1, y = V2, color= Grupo))
```

]
]

---


## KNN: Primeiros passos


```r
# Carregando os pacotes
library(tidymodels)
library(kknn)
# Especificamos o modelo
*knn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;%
            set_engine("kknn") %&gt;% 
            set_mode("classification") 
# Ajustamos o modelo (fit)
knn_fit &lt;- knn_spec %&gt;% 
           fit(factor(Grupo) ~ V1 + V2, data = knn_exemplo)
# Classificando new_obs (predict)
new_obs &lt;- data.frame(V1 = 0.7, V2 = 0.4)
yhat &lt;- predict(knn_fit, new_data = new_obs)
```

--

- Rodar o mesmo c√≥digo mas com outro valor de `\(k\)` (mudar o argumento `neighbors`). Os resultados s√£o os mesmos?

--


```r
yhat
```

```
## # A tibble: 1 x 1
##   .pred_class
##   &lt;fct&gt;      
## 1 B
```


---

## KNN: Como o algoritmo funciona?


.pull-left[

.blue[

### Pseudoc√≥digo

1. Escolher uma m√©trica `\(d(\cdot, \cdot)\)` para medir a dist√¢ncia entre os pontos.
2. Calcular a dist√¢ncia do ponto a classificar `\(x_0\)` a todos os pontos na amostra de treinamento `$$d(x_0, x_i) \quad \forall i = 1, \cdots, n$$`
3. Selecionar os `\(k\)`-vizinhos mais pr√≥ximos a `\(x_0\)`  (aqueles `\(x_i\)` com os `\(k\)` menores valores de `\(d(x_0, x_i)\)`)
4. Classificar `\(x_0\)` no grupo com maior frequ√™ncia entre os `\(k\)`-vizinhos mais pr√≥ximos
]
]

--

.pull-right[

### Exemplo

Sejam os pontos `\(P\)` e `\(Q\)` em que:

- `\(P = (p_1, \ldots, p_n)\)`
- `\(Q = (q_1, \ldots, q_n)\)`


A dist√¢ncia euclidiana entre `\(P\)` e `\(Q\)` √© definida como: .red[$$d(P,Q) = \sqrt{(p_1-q_1)^2 + \cdots + (p_n-q_n)^2}$$]


No _dataset_ `knn_exemplo`, a nova observa√ß√£o √© `\(x_0\)` (`new_obs = data.frame(V1 = 0.7, V2 = 0.4)`), vamos a calcular as dist√¢ncias para entender como o algoritmo funciona.

]

---

## KNN: Como o algoritmo funciona?


|  V1|  V2|Grupo |         d|
|---:|---:|:-----|---------:|
| 0.7| 0.5|B     | 0.1000000|
| 0.8| 0.5|B     | 0.1414214|
| 0.6| 0.6|A     | 0.2236068|
| 0.5| 0.5|A     | 0.2236068|
| 0.8| 0.6|B     | 0.2236068|
| 0.5| 0.6|A     | 0.2828427|
| 0.7| 0.7|A     | 0.3000000|
| 0.8| 0.7|B     | 0.3162278|
| 1.0| 0.5|B     | 0.3162278|
| 1.0| 0.3|B     | 0.3162278|
| 0.5| 0.7|A     | 0.3605551|
| 1.0| 0.6|B     | 0.3605551|
| 0.7| 0.8|A     | 0.4000000|
| 0.5| 0.8|A     | 0.4472136|

---
class: inverse, center, middle
# KNN: Case
---

## KNN: Case


![Breast Cancer Wisconsin dataset](imagens/cancer.png)

--

Para mais _datasets_ para praticar, podem entrar no _UCI Machine Learning Repository_ [aqui](http://archive.ics.uci.edu/ml/datasets.php)


---

## KNN: Case


.panelset[
.panel[.panel-name[Importando dados]


```r
# Importando os dados
uri &lt;- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
data &lt;- read.table(uri, sep=",")
glimpse(data)
```
]

.panel[.panel-name[Pr√©-processamento]


```r
data &lt;- data %&gt;% select(-V1) %&gt;% rename(Diagnostico = V2) %&gt;% na.omit()
dim(data)
```
]

.panel[.panel-name[Splitting data]


```r
# Spliting data
*set.seed(1234)
data_split &lt;- initial_split(data, prop = 3/4, strata = Diagnostico)
train_data &lt;- training(data_split)
test_data &lt;- testing(data_split)
x_test_data &lt;- test_data %&gt;% select(-Diagnostico)
y_test_data &lt;- test_data %&gt;% select(Diagnostico)
```
]

.panel[.panel-name[KNN]

```r
library(tidymodels)
library(kknn)
# Especificamos o modelo
*knn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;%
            set_engine("kknn") %&gt;% 
            set_mode("classification") 
# Ajustamos o modelo (fit)
knn_fit &lt;- knn_spec %&gt;% 
           fit(factor(Diagnostico) ~ ., data = train_data)
# Clasificando as novas observa√ß√µes (predict)
yhat_test &lt;- predict(knn_fit, new_data = x_test_data)
table(yhat_test$.pred_class,y_test_data$Diagnostico)
```
]

.panel[.panel-name[Avaliando o modelo]


```
##    
##      B  M
##   B 88  5
##   M  2 48
```


- A matriz acima √© conhecida como matriz de confus√£o e nos da informa√ß√£o sobre que t√£o bem foi feita a classifica√ß√£o. No futuro entraremos em maior detalhes sobre essa matriz.
- Por agora, nos preocuparemos unicamente em maximizar a soma dos elementos da diagional .blue[(observa√ß√µes corretamente classificadas).]




]
]


---

## KNN: Como escolher K?

- Uma forma √© utilizando _cross-validation_ (veremos isto no futuro) 
- Pe√±a (2002) sugere utilizar `\(k = \sqrt{n_g},\)` em que `\(n_g\)` √© o tamanho m√©dios dos grupos.


```r
table(train_data$Diagnostico)
m = floor(sqrt(mean(table(train_data$Diagnostico))))
knn_spec &lt;- nearest_neighbor(neighbors = m) %&gt;% set_engine("kknn") %&gt;% set_mode("classification") 
# Ajustamos o modelo (fit)
knn_fit &lt;- knn_spec %&gt;% fit(factor(Diagnostico) ~ ., data = train_data)
yhat_test &lt;- predict(knn_fit, new_data = x_test_data)
table(yhat_test$.pred_class,y_test_data$Diagnostico)
```



---


                  
## Data-Tips:


.pull-left[ 
![](https://octodex.github.com/images/minertocat.png)
]
.pull-right[

- Utilize outros conjuntos de dados do [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets.php)

- A implementa√ß√£o do kknn utiliza a dist√¢ncia de Minkowski (p = 2), se as vari√°veis estiverem em unidades de medida diferentes n√£o calcular√° a dist√¢ncia de forma _justa_. Neste caso uma boa op√ß√£o √© padronizar as vari√°veis (f√°cil) ou utilizar a dist√¢ncia de Mahalanobis (n√£o implementada no kknn mas em outros pacotes do R).

Minkowski: 
.red[$$d(P,Q) = \Big(|p_1-q_1|^p + \cdots + |p_n-q_n|^p \Big)^{1/p}$$]

Mahalanobis: Seja `\(S\)` a matriz de covari√¢ncia entre `\(P\)` e `\(Q\)`,
.red[$$d(P,Q) = \sqrt{(P-Q)'S^{-1}(P-Q)}$$]

]


---

## Refer√™ncias:


 
- Pe√±a, D. (2002). An√°lisis de datos multivariantes. Madrid: McGraw-hill. Cap√≠tulo 14.5.3
- [James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. New York: Springer.](https://www.statlearning.com) Chapter 2
- [Chatzidimitriou, K., Diamantopoulos, T., Papamichail, M., and Symeonidis, A. (2018). Practical Machine Learning in R. Leanpub.](https://leanpub.com/practical-machine-learning-r) Chapter 5


## Nota

Aqui temos visto o algoritmo KNN para classifica√ß√£o. Contudo, pode tamb√©m ser utilizado para problemas de regress√£o.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
